@page "/web-rtc"
@rendermode InteractiveServer
@using System.Text.Json
@using BlazorVoice.Akka
@using BlazorVoice.Akka.Actor
@using global::Akka.Actor
@inject BlazorVoice.Services.OpenAIService OpenAIService
@inject IJSRuntime JSRuntime
@inject AkkaService AkkaService
@inject IServiceProvider ServiceProvider

<PageTitle>AIVoiceChat</PageTitle>

<h1>AIVoiceChat</h1>


<style>
    .webrtc-root {
        display: flex;
        flex-direction: column;
        height: 100vh;
        min-height: 0;
    }

    .webrtc-main {
        flex: 1 1 0;
        display: flex;
        min-height: 0;
        overflow: hidden;
    }

    .webrtc-player {
        width: 420px;
        min-width: 320px;
        max-width: 480px;
        background: #f8f8f8;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: flex-start;
        padding: 16px 0;
        border-right: 1px solid #eee;
    }

    .webrtc-chat {
        flex: 1 1 0;
        display: flex;
        flex-direction: column;
        min-width: 0;
        height: 100%;
    }

    .webrtc-chat-list {
        flex: 1 1 0;
        overflow-y: auto;
        min-height: 0;
        padding: 16px;
        background: #fff;
    }

    .webrtc-chat-input {
        padding: 16px;
        background: #fafafa;
        border-top: 1px solid #eee;
    }

    .webrtc-audio-bar {
        width: 100%;
        background: #f5f5f5;
        border-top: 1px solid #ddd;
        padding: 12px 24px;
    }
</style>

<div class="webrtc-root">
    <div class="webrtc-main">
        <!-- Player 영역 (왼쪽 고정) -->
        <div class="webrtc-player">
            <MudPaper Class="d-flex align-center justify-center mud-width-full mb-4" Style="background:transparent;">
                <canvas id="aiFaceCanvas" width="320" height="240" style="pointer-events: none;"></canvas>
            </MudPaper>            
            <MudPaper Class="d-flex align-center justify-center mud-width-full" Style="background:transparent;">
                <video id="localVideo" autoplay muted style="width: 50%; border-radius: 8px;"></video>
            </MudPaper>
        </div>

        <!-- 채팅창 (오른쪽, 스크롤) -->
        <div class="webrtc-chat">
            <div class="webrtc-chat-list">
                <MudText Typo="Typo.subtitle1">채팅</MudText>
                <MudList T="string">
                    @foreach (var chat in ChatMessages)
                    {
                        <MudListItem Text="@chat" Icon="@Icons.Material.Filled.Chat" />
                    }
                </MudList>
            </div>
            <div class="webrtc-chat-input">
                <MudStack Row="true" Spacing="2">
                    <MudTextField @bind-Value="ChatInput" Placeholder="메시지를 입력하세요..." Style="flex:1"
                                  OnKeyUp="HandleChatInputKeyUp" />
                    <MudButton OnClick="SendChatMessage" >전송</MudButton>
                </MudStack>
            </div>
        </div>
    </div>

    <!-- Audio 컨트롤 (항상 하단) -->
    <div class="webrtc-audio-bar">
        <MudStack>
            <MudButton OnClick="StartWebRTC" Class="mt-4 mud-width-full">
                Start WebRTC
            </MudButton>
            <MudText Typo="Typo.subtitle1">오디오바</MudText>
            <MudProgressLinear Value="@AudioLevel" Class="mt-2" />
            <MudText Typo="Typo.subtitle1" Class="mt-4">마이크 볼륨 조절</MudText>
            <MudSlider @bind-Value="MicrophoneVolume" Min="0" Max="100" Step="1" Class="mt-2" />
        </MudStack>
    </div>
</div>


@code {

    private double AudioLevel { get; set; } = 0;
    private double MicrophoneVolume { get; set; } = 10; // 초기 볼륨 값
    private string ChatInput { get; set; } = string.Empty;
    private List<string> ChatMessages { get; set; } = new();
    private string SelectedChatOption { get; set; } = "EchoTTS";
    private string UserId { get; set; } = string.Empty;

    private IActorRef MyVoiceActor { get; set; } = null!;



    protected override async Task OnAfterRenderAsync(bool firstRender)
    {        
        if (firstRender)
        {
            // Live2D 모델 초기화 (모델 경로는 실제 파일 위치에 맞게 수정)
            await JSRuntime.InvokeVoidAsync(
                "initLive2D"
            );

            // JavaScript를 호출하여 유니크 ID를 가져옴
            UserId = await JSRuntime.InvokeAsync<string>("getOrCreateUniqueId");
            Console.WriteLine($"User ID: {UserId}");

            string voiceChatActorKey = $"VoiceChatActor-{UserId}";

            var actorSystem = AkkaService.GetActorSystem();

            var voiceCahtActor = AkkaService.GetActor(voiceChatActorKey);

            if (voiceCahtActor == null)
            {
                // VoiceChatActor가 없으면 생성 - 비식별 세션별단위로 생성
                voiceCahtActor = actorSystem.ActorOf(Props.Create(() =>
                    new VoiceChatActor(ServiceProvider)), UserId);

                AkkaService.AddActor(voiceChatActorKey, voiceCahtActor);
            }

            // UX를 제어권인 대리자 생성해 전달
            voiceCahtActor.Tell(CreateDynamicDelegate());

            MyVoiceActor = voiceCahtActor;

        }
    }

    protected override async Task OnInitializedAsync()
    {


    }

    private async Task StartWebRTC()
    {
        var dotNetRef = DotNetObjectReference.Create(this);
        await JSRuntime.InvokeVoidAsync("startWebRTC", "localVideo", "remoteVideo", dotNetRef);
    }


    private const int BufferDurationMs = 3000; // 3초
    private List<double> audioLevelBuffer = new();
    private List<byte> audioDataBuffer = new();
    private DateTime lastBufferTime = DateTime.UtcNow;

    // 동적 잡음 기준선(Noise Floor) 및 음성 감지 임계값
    private double noiseFloor = 0;
    private const double VoiceThresholdOffset = 8.0; // 잡음보다 8 이상 크면 음성으로 간주
    private int silenceCount = 0;
    private int voiceCount = 0;
    private const int MinVoiceFrames = 5; // 최소 연속 음성 프레임 수

    private const int MinAudioSeconds = 5;
    private const int SampleRate = 48000; // JS AudioContext 기본값과 일치시켜야 함
    private const int Channels = 1;
    private const int BytesPerSample = 4; // Float32Array



    [JSInvokable]
    public async Task SendAudioData(string audioDataJson)
    {
        try
        {
            // Parse JSON data
            var data = JsonSerializer.Deserialize<Dictionary<string, object>>(audioDataJson);
            if (data != null && data.ContainsKey("volume") && data.ContainsKey("audioData"))
            {
                // Validate volume
                if (data["volume"] is JsonElement volumeElement && volumeElement.TryGetDouble(out var volume))
                {
                    // Validate audioData
                    if (data["audioData"] is JsonElement audioDataElement && audioDataElement.ValueKind == JsonValueKind.Array)
                    {
                        var audioBytes = audioDataElement.EnumerateArray().Select(x => (byte)x.GetInt32()).ToArray();

                        // 잡음 기준선 업데이트 (최근 3초간 최소값의 이동평균)
                        audioLevelBuffer.Add(volume);
                        if (audioLevelBuffer.Count > 30)
                            audioLevelBuffer.RemoveAt(0);
                        noiseFloor = audioLevelBuffer.Min();

                        // 음성 감지
                        if (volume > noiseFloor + VoiceThresholdOffset)
                        {
                            voiceCount++;
                            silenceCount = 0;
                            audioDataBuffer.AddRange(audioBytes);
                        }
                        else
                        {
                            silenceCount++;
                            // 최소 5초 이상 데이터가 쌓였을 때만 PlayByte 호출
                            //int minLength = MinAudioSeconds * SampleRate * Channels * BytesPerSample;
                            if (voiceCount > 150)
                            {
                                //await PlayByte(audioDataBuffer.ToArray());
                                audioDataBuffer.Clear();
                                voiceCount = 0;
                            }                            
                        }

                        // Update UI with volume
                        AudioLevel = Math.Clamp(volume, 0, 100);
                        StateHasChanged();
                        // Debugging output
                        //Console.WriteLine($"Valid audio data received. Volume: {volume}, Data size: {audioBytes.Length} bytes");
                        // Send audio data to SignalR server (if needed)
                        // await HubContext.Clients.All.SendAsync("SendAudioData", audioBytes);
                    }
                    else
                    {
                        Console.WriteLine("Invalid audioData format.");
                    }
                }
                else
                {
                    Console.WriteLine("Invalid volume format.");
                }
            }
            else
            {
                Console.WriteLine("Invalid audio data structure.");
            }
        }
        catch (Exception ex)
        {
            Console.WriteLine($"Error in SendAudioData: {ex.Message}");
        }
    }


    private async Task UpdateMicrophoneVolume()
    {
        await JSRuntime.InvokeVoidAsync("setMicrophoneVolume", MicrophoneVolume);
    }

    private async Task HandleChatInputKeyUp(KeyboardEventArgs e)
    {
        if (e.Key == "Enter")
        {
            await SendChatMessage();
        }
    }

    private async Task SendChatMessage()
    {
        if (!string.IsNullOrWhiteSpace(ChatInput))
        {
            await HandleTTS(ChatInput);

            ChatInput = string.Empty;            

        }
    }

    private async Task HandleTTS(string message)
    {
        MyVoiceActor.Tell(new TTSCommand()
        {
            From = "Your",
            Text = message,
            Voice = "coral"
        });        

        StateHasChanged(); // UI 업데이트
    }

    [JSInvokable]
    public async Task OnAudioPlaybackCompleted(int option)
    {
        MyVoiceActor.Tell(new TTSCommand()
        {
            From = "AI",
            Text = "LLM자동재생",
            Voice = "alloy"
        });
    }

    private void PlayAudioBytes(float[] voice, float speed, int playtype)
    {
        InvokeAsync(() =>
        {
            var dotNetRef = DotNetObjectReference.Create(this);
            JSRuntime.InvokeVoidAsync("playAudioBytes", voice, speed, playtype, dotNetRef);
        });
    }


    private void AddMessage(string from, string message)
    {
        InvokeAsync(() =>
        {
            ChatMessages.Add($"[{from}] {message} - {DateTime.Now:HH:mm:ss}");
            StateHasChanged(); // UI 업데이트
        });        
    }

    private Action<string, object[]> CreateDynamicDelegate()
    {
        return (methodName, args) =>
        {
            // Blazor 컴포넌트의 메서드 찾기
            var method = GetType().GetMethod(methodName, System.Reflection.BindingFlags.Instance | System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Public);
            if (method == null)
            {
                Console.WriteLine($"Method '{methodName}' not found.");
                return;
            }

            try
            {
                // 메서드 호출
                method.Invoke(this, args);
            }
            catch (Exception ex)
            {
                Console.WriteLine($"Error invoking method '{methodName}': {ex.Message}");
            }
        };
    }

}
